{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "from langchain.schema import Document\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from typing import List, Dict, Any"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataSet PReprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Operator': 'Verizon',\n",
       "  'Network Function': 'ADPF',\n",
       "  'Dimensioning Flavor': 'medium-tdd-regular-spr',\n",
       "  'Package': '25A',\n",
       "  'DPP': 'medium-tdd-regular-spr',\n",
       "  'DIP': 'medium-1.8m-spr',\n",
       "  'DMP': 'medium-uni',\n",
       "  'CMP': 'medium-uni',\n",
       "  'PMP': 'medium-uni',\n",
       "  'RMP': 'medium-uni',\n",
       "  'IPP': nan},\n",
       " {'Operator': 'VOS',\n",
       "  'Network Function': 'uADPF',\n",
       "  'Dimensioning Flavor': 'medium-regular-spr-t21',\n",
       "  'Package': '25A',\n",
       "  'DPP': 'fdd-270m-18c-gsm-6trx-spr',\n",
       "  'DIP': 'medium-uni',\n",
       "  'DMP': 'medium-uni',\n",
       "  'CMP': 'medium-uni',\n",
       "  'PMP': 'medium-uni',\n",
       "  'RMP': 'medium-uni',\n",
       "  'IPP': 'medium'},\n",
       " {'Operator': 'VOS',\n",
       "  'Network Function': 'uADPF',\n",
       "  'Dimensioning Flavor': 'small-tdd-6fh-3fhm',\n",
       "  'Package': '25B',\n",
       "  'DPP': 'small-tdd',\n",
       "  'DIP': 'tiny',\n",
       "  'DMP': 'medium',\n",
       "  'CMP': 'medium',\n",
       "  'PMP': 'medium',\n",
       "  'RMP': 'medium',\n",
       "  'IPP': nan},\n",
       " {'Operator': 'VOS',\n",
       "  'Network Function': 'uADPF',\n",
       "  'Dimensioning Flavor': 'medium-regular-spr-t20',\n",
       "  'Package': '25B',\n",
       "  'DPP': 'medium-slim-gsm-spr',\n",
       "  'DIP': 'medium-uni',\n",
       "  'DMP': 'medium-uni',\n",
       "  'CMP': 'medium-uni',\n",
       "  'PMP': 'medium-uni',\n",
       "  'RMP': 'medium-uni',\n",
       "  'IPP': 'medium'},\n",
       " {'Operator': 'VOS',\n",
       "  'Network Function': 'uADPF',\n",
       "  'Dimensioning Flavor': 'medium-regular-spr-t22',\n",
       "  'Package': '25B',\n",
       "  'DPP': 'medium-slim-gsm-spr',\n",
       "  'DIP': 'medium-uni',\n",
       "  'DMP': 'medium-uni',\n",
       "  'CMP': 'medium-uni',\n",
       "  'PMP': 'medium-uni',\n",
       "  'RMP': 'medium-uni',\n",
       "  'IPP': 'medium'},\n",
       " {'Operator': 'VOS',\n",
       "  'Network Function': 'uADPF',\n",
       "  'Dimensioning Flavor': 'medium-tdd-spr-t20',\n",
       "  'Package': '25B',\n",
       "  'DPP': 'tdd-510m-9c-spr',\n",
       "  'DIP': 'medium-uni',\n",
       "  'DMP': 'medium-uni',\n",
       "  'CMP': 'medium-uni',\n",
       "  'PMP': 'medium-uni',\n",
       "  'RMP': 'medium-uni',\n",
       "  'IPP': 'medium'},\n",
       " {'Operator': 'VOS',\n",
       "  'Network Function': 'uADPF',\n",
       "  'Dimensioning Flavor': 'medium-regular-spr-t21',\n",
       "  'Package': '25B',\n",
       "  'DPP': 'fdd-270m-18c-gsm-9trx-spr',\n",
       "  'DIP': 'medium-uni',\n",
       "  'DMP': 'medium-uni',\n",
       "  'CMP': 'medium-uni',\n",
       "  'PMP': 'medium-uni',\n",
       "  'RMP': 'medium-uni',\n",
       "  'IPP': 'small'},\n",
       " {'Operator': 'VOS',\n",
       "  'Network Function': 'uADPF',\n",
       "  'Dimensioning Flavor': 'medium-regular-spr-t23',\n",
       "  'Package': '25B',\n",
       "  'DPP': 'fdd-120m-12c-gsm-8trx-spr',\n",
       "  'DIP': 'medium-uni',\n",
       "  'DMP': 'medium-uni',\n",
       "  'CMP': 'medium-uni',\n",
       "  'PMP': 'medium-uni',\n",
       "  'RMP': 'medium-uni',\n",
       "  'IPP': 'medium'},\n",
       " {'Operator': 'VOS',\n",
       "  'Network Function': 'uADPF',\n",
       "  'Dimensioning Flavor': 'small-tdd-spr-t20',\n",
       "  'Package': '25B',\n",
       "  'DPP': 'tdd-300m-3c-spr',\n",
       "  'DIP': 'medium-uni',\n",
       "  'DMP': 'medium-uni',\n",
       "  'CMP': 'medium-uni',\n",
       "  'PMP': 'medium-uni',\n",
       "  'RMP': 'medium-uni',\n",
       "  'IPP': 'medium'},\n",
       " {'Operator': 'VOS',\n",
       "  'Network Function': 'uADPF',\n",
       "  'Dimensioning Flavor': 'medium-regular-sn-t20',\n",
       "  'Package': '25B',\n",
       "  'DPP': 'medium-regular-sn-t20-xaca',\n",
       "  'DIP': 'medium-2m',\n",
       "  'DMP': 'medium-uni',\n",
       "  'CMP': 'medium-uni',\n",
       "  'PMP': 'medium-uni',\n",
       "  'RMP': 'medium-uni',\n",
       "  'IPP': nan},\n",
       " {'Operator': 'VOS',\n",
       "  'Network Function': 'uADPF',\n",
       "  'Dimensioning Flavor': 'medium-regular-gnr-t20',\n",
       "  'Package': '25B',\n",
       "  'DPP': 'medium-regular-gsm-gnr-t20',\n",
       "  'DIP': 'medium-2m',\n",
       "  'DMP': 'medium-uni',\n",
       "  'CMP': 'medium-uni',\n",
       "  'PMP': 'medium-uni',\n",
       "  'RMP': 'medium-uni',\n",
       "  'IPP': 'small'},\n",
       " {'Operator': 'VOS',\n",
       "  'Network Function': 'uADPF',\n",
       "  'Dimensioning Flavor': 'medium-tdd-gnr-t20',\n",
       "  'Package': '25B',\n",
       "  'DPP': 'medium-tdd-gnr-t20',\n",
       "  'DIP': 'medium-2m',\n",
       "  'DMP': 'medium-uni',\n",
       "  'CMP': 'medium-uni',\n",
       "  'PMP': 'medium-uni',\n",
       "  'RMP': 'medium-uni',\n",
       "  'IPP': 'large'},\n",
       " {'Operator': 'VOS',\n",
       "  'Network Function': 'uADPF',\n",
       "  'Dimensioning Flavor': 'medium-regular-gnr-t22',\n",
       "  'Package': '26A',\n",
       "  'DPP': 'medium-regular-gnr-t22',\n",
       "  'DIP': 'medium-2m',\n",
       "  'DMP': 'medium-uni',\n",
       "  'CMP': 'medium-uni',\n",
       "  'PMP': 'medium-uni',\n",
       "  'RMP': 'medium-uni',\n",
       "  'IPP': 'small'},\n",
       " {'Operator': 'VOS',\n",
       "  'Network Function': 'uADPF',\n",
       "  'Dimensioning Flavor': 'medium-regular-gnr-mnode-2-t22',\n",
       "  'Package': '26A',\n",
       "  'DPP': 'medium-regular-gnr-t22',\n",
       "  'DIP': 'medium-2m',\n",
       "  'DMP': 'medium-uni',\n",
       "  'CMP': 'medium-uni',\n",
       "  'PMP': 'medium-uni',\n",
       "  'RMP': 'medium-uni',\n",
       "  'IPP': 'small'},\n",
       " {'Operator': 'Verizon',\n",
       "  'Network Function': 'ADPF',\n",
       "  'Dimensioning Flavor': 'small-tdd',\n",
       "  'Package': '25B',\n",
       "  'DPP': 'small-tdd',\n",
       "  'DIP': 'tiny',\n",
       "  'DMP': 'medium',\n",
       "  'CMP': 'medium',\n",
       "  'PMP': 'medium',\n",
       "  'RMP': 'medium',\n",
       "  'IPP': nan},\n",
       " {'Operator': 'Verizon',\n",
       "  'Network Function': 'ADPF',\n",
       "  'Dimensioning Flavor': 'small-tdd-light',\n",
       "  'Package': '25B',\n",
       "  'DPP': 'small-tdd-light-6c',\n",
       "  'DIP': 'tiny',\n",
       "  'DMP': 'medium',\n",
       "  'CMP': 'medium',\n",
       "  'PMP': 'medium',\n",
       "  'RMP': 'medium',\n",
       "  'IPP': nan},\n",
       " {'Operator': 'Verizon',\n",
       "  'Network Function': 'ADPF',\n",
       "  'Dimensioning Flavor': 'medium-tdd-light',\n",
       "  'Package': '25B',\n",
       "  'DPP': 'medium-tdd-light-db-9c',\n",
       "  'DIP': 'medium-1m',\n",
       "  'DMP': 'medium',\n",
       "  'CMP': 'medium',\n",
       "  'PMP': 'medium',\n",
       "  'RMP': 'medium',\n",
       "  'IPP': nan},\n",
       " {'Operator': 'Verizon',\n",
       "  'Network Function': 'ADPF',\n",
       "  'Dimensioning Flavor': 'medium-tdd-light-6sector',\n",
       "  'Package': '25B',\n",
       "  'DPP': 'medium-tdd-light-100m12c',\n",
       "  'DIP': 'medium-1m',\n",
       "  'DMP': 'medium',\n",
       "  'CMP': 'medium',\n",
       "  'PMP': 'medium',\n",
       "  'RMP': 'medium',\n",
       "  'IPP': nan},\n",
       " {'Operator': 'Verizon',\n",
       "  'Network Function': 'ADPF',\n",
       "  'Dimensioning Flavor': 'medium',\n",
       "  'Package': '25B',\n",
       "  'DPP': 'medium',\n",
       "  'DIP': 'medium-1mmedium-1m',\n",
       "  'DMP': 'medium',\n",
       "  'CMP': 'medium',\n",
       "  'PMP': 'medium',\n",
       "  'RMP': 'medium',\n",
       "  'IPP': nan},\n",
       " {'Operator': 'Verizon',\n",
       "  'Network Function': 'ADPF',\n",
       "  'Dimensioning Flavor': 'medium-icl',\n",
       "  'Package': '25B',\n",
       "  'DPP': 'medium-icl',\n",
       "  'DIP': 'medium-icl',\n",
       "  'DMP': 'medium',\n",
       "  'CMP': 'medium',\n",
       "  'PMP': 'medium',\n",
       "  'RMP': 'medium',\n",
       "  'IPP': nan},\n",
       " {'Operator': 'Verizon',\n",
       "  'Network Function': 'ADPF',\n",
       "  'Dimensioning Flavor': 'medium-icl-mnode',\n",
       "  'Package': '25B',\n",
       "  'DPP': 'medium-icl',\n",
       "  'DIP': 'medium-2m',\n",
       "  'DMP': 'medium',\n",
       "  'CMP': 'medium',\n",
       "  'PMP': 'medium',\n",
       "  'RMP': 'medium',\n",
       "  'IPP': nan},\n",
       " {'Operator': 'Verizon',\n",
       "  'Network Function': 'ADPF',\n",
       "  'Dimensioning Flavor': 'medium-tdd-regular-spr ',\n",
       "  'Package': '25B',\n",
       "  'DPP': 'medium-tdd-regular-spr',\n",
       "  'DIP': 'medium-1.8m-spr',\n",
       "  'DMP': 'medium-uni',\n",
       "  'CMP': 'medium-uni',\n",
       "  'PMP': 'medium-uni',\n",
       "  'RMP': 'medium-uni',\n",
       "  'IPP': nan},\n",
       " {'Operator': 'Verizon',\n",
       "  'Network Function': 'uADPF',\n",
       "  'Dimensioning Flavor': 'medium-uni',\n",
       "  'Package': '25B',\n",
       "  'DPP': 'medium-uni',\n",
       "  'DIP': 'medium-uni',\n",
       "  'DMP': 'medium-uni-pooling',\n",
       "  'CMP': 'medium-uni-pooling',\n",
       "  'PMP': 'medium-uni-pooling',\n",
       "  'RMP': 'medium-uni-pooling',\n",
       "  'IPP': nan},\n",
       " {'Operator': 'Verizon',\n",
       "  'Network Function': 'uADPF',\n",
       "  'Dimensioning Flavor': 'medium-uni-regular-spr ',\n",
       "  'Package': '25B',\n",
       "  'DPP': 'medium-uni-regular-spr',\n",
       "  'DIP': 'medium-uni-spr',\n",
       "  'DMP': 'medium-uni',\n",
       "  'CMP': 'medium-uni',\n",
       "  'PMP': 'medium-uni',\n",
       "  'RMP': 'medium-uni',\n",
       "  'IPP': nan},\n",
       " {'Operator': 'Verizon',\n",
       "  'Network Function': 'uADPF',\n",
       "  'Dimensioning Flavor': 'medium-uni-light-spr ',\n",
       "  'Package': '25B',\n",
       "  'DPP': 'medium-uni-light-spr',\n",
       "  'DIP': 'medium-uni',\n",
       "  'DMP': 'medium-uni',\n",
       "  'CMP': 'medium-uni',\n",
       "  'PMP': 'medium-uni',\n",
       "  'RMP': 'medium-uni',\n",
       "  'IPP': nan},\n",
       " {'Operator': 'Verizon',\n",
       "  'Network Function': 'uADPF',\n",
       "  'Dimensioning Flavor': 'medium-uni-light-spr-enterprise',\n",
       "  'Package': '25B',\n",
       "  'DPP': 'medium-uni-light-spr-enterprise',\n",
       "  'DIP': 'tiny-spr',\n",
       "  'DMP': 'medium-uni',\n",
       "  'CMP': 'medium-uni',\n",
       "  'PMP': 'medium-uni',\n",
       "  'RMP': 'medium-uni',\n",
       "  'IPP': nan},\n",
       " {'Operator': 'Verizon',\n",
       "  'Network Function': 'uADPF',\n",
       "  'Dimensioning Flavor': 'medium-uni-light-gnr-hcc',\n",
       "  'Package': '25B',\n",
       "  'DPP': 'medium-uni-light-gnr-hcc',\n",
       "  'DIP': 'tiny-1.2m-gnr',\n",
       "  'DMP': 'medium-uni',\n",
       "  'CMP': 'medium-uni',\n",
       "  'PMP': 'medium-uni',\n",
       "  'RMP': 'medium-uni',\n",
       "  'IPP': nan},\n",
       " {'Operator': 'Verizon',\n",
       "  'Network Function': 'uADPF',\n",
       "  'Dimensioning Flavor': 'medium-uni-regular-gnr-xcc',\n",
       "  'Package': '25B',\n",
       "  'DPP': 'medium-uni-regular-gnr-xcc',\n",
       "  'DIP': 'medium-3.6m-gnr',\n",
       "  'DMP': 'medium-uni',\n",
       "  'CMP': 'medium-uni',\n",
       "  'PMP': 'medium-uni',\n",
       "  'RMP': 'medium-uni',\n",
       "  'IPP': nan},\n",
       " {'Operator': 'PNU',\n",
       "  'Network Function': 'uADPF',\n",
       "  'Dimensioning Flavor': 'medium-uni-enterprise',\n",
       "  'Package': '25B',\n",
       "  'DPP': 'medium',\n",
       "  'DIP': 'medium',\n",
       "  'DMP': 'medium',\n",
       "  'CMP': 'medium',\n",
       "  'PMP': 'medium',\n",
       "  'RMP': 'medium',\n",
       "  'IPP': nan},\n",
       " {'Operator': 'BM',\n",
       "  'Network Function': 'uADPF',\n",
       "  'Dimensioning Flavor': 'medium-uni-regular-spr ',\n",
       "  'Package': '25B',\n",
       "  'DPP': 'medium',\n",
       "  'DIP': 'medium',\n",
       "  'DMP': 'medium',\n",
       "  'CMP': 'medium',\n",
       "  'PMP': 'medium',\n",
       "  'RMP': 'medium',\n",
       "  'IPP': nan},\n",
       " {'Operator': 'BM',\n",
       "  'Network Function': 'uADPF',\n",
       "  'Dimensioning Flavor': 'medium-regular-t1',\n",
       "  'Package': '25B',\n",
       "  'DPP': 'medium',\n",
       "  'DIP': 'medium',\n",
       "  'DMP': 'medium',\n",
       "  'CMP': 'medium',\n",
       "  'PMP': 'medium',\n",
       "  'RMP': 'medium',\n",
       "  'IPP': nan},\n",
       " {'Operator': 'BM',\n",
       "  'Network Function': 'uADPF',\n",
       "  'Dimensioning Flavor': 'medium-regular-t3',\n",
       "  'Package': '25B',\n",
       "  'DPP': 'medium',\n",
       "  'DIP': 'medium',\n",
       "  'DMP': 'medium',\n",
       "  'CMP': 'medium',\n",
       "  'PMP': 'medium',\n",
       "  'RMP': 'medium',\n",
       "  'IPP': nan},\n",
       " {'Operator': 'BM',\n",
       "  'Network Function': 'uADPF',\n",
       "  'Dimensioning Flavor': 'medium-regular-t5',\n",
       "  'Package': '25B',\n",
       "  'DPP': 'medium',\n",
       "  'DIP': 'medium',\n",
       "  'DMP': 'medium',\n",
       "  'CMP': 'medium',\n",
       "  'PMP': 'medium',\n",
       "  'RMP': 'medium',\n",
       "  'IPP': nan},\n",
       " {'Operator': 'BM',\n",
       "  'Network Function': 'uADPF',\n",
       "  'Dimensioning Flavor': 'medium-regular-t7',\n",
       "  'Package': '25B',\n",
       "  'DPP': 'medium',\n",
       "  'DIP': 'medium',\n",
       "  'DMP': 'medium',\n",
       "  'CMP': 'medium',\n",
       "  'PMP': 'medium',\n",
       "  'RMP': 'medium',\n",
       "  'IPP': nan},\n",
       " {'Operator': 'Boost',\n",
       "  'Network Function': 'uADPF',\n",
       "  'Dimensioning Flavor': 'medium-uni-regular-gnr-xcc',\n",
       "  'Package': '26A',\n",
       "  'DPP': 'medium-uni-regular-gnr-xcc',\n",
       "  'DIP': 'medium-3.6m-gnr',\n",
       "  'DMP': 'medium-uni',\n",
       "  'CMP': 'medium-uni',\n",
       "  'PMP': 'medium-uni',\n",
       "  'RMP': 'medium-uni',\n",
       "  'IPP': nan}]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the Excel sheets\n",
    "dimensioning_df = pd.read_csv(\"dimension_flavor_25A_25B_26A.csv\")\n",
    "\n",
    "df_docStrList = []\n",
    "df_map_list = []\n",
    "\n",
    "for idx, row in dimensioning_df.iterrows():\n",
    "    map={}\n",
    "    operator = row['Operator']\n",
    "    network_function = row['Network Function']\n",
    "    dimensioning_flavour = row['Dimensioning Flavor']\n",
    "    package = row['Package']\n",
    "    dpp = row['DPP']\n",
    "    dip = row['DIP']\n",
    "    dmp = row['DMP']\n",
    "    cmp = row['CMP']\n",
    "    pmp = row['PMP']\n",
    "    rmp = row['RMP']\n",
    "    ipp = row['IPP']\n",
    "    map.update({'Operator': operator, 'Network Function': network_function, 'Dimensioning Flavor': dimensioning_flavour, 'Package': package, 'DPP': dpp, 'DIP': dip, 'DMP': dmp, 'CMP': cmp, 'PMP': pmp, 'RMP': rmp, 'IPP': ipp})\n",
    "    df_map_list.append(map)\n",
    "    content = f\"Operator: {operator}\\n\"\n",
    "    content += f\"Network Function: {network_function}\\n\"\n",
    "    content += f\"Dimensioning Flavour: {dimensioning_flavour}\\n\"\n",
    "    content += f\"Package: {package}\\n\"\n",
    "    content += f\"DPP: {dpp}\\n\"\n",
    "    content += f\"DIP: {dip}\\n\"\n",
    "    content += f\"DMP: {dmp}\\n\"\n",
    "    content += f\"CMP: {cmp}\\n\"\n",
    "    content += f\"PMP: {pmp}\\n\"\n",
    "    content += f\"RMP: {rmp}\\n\"\n",
    "    content += f\"IPP: {ipp}\\n\"\n",
    "\n",
    "    df_docStrList.append(content.strip())\n",
    "\n",
    "df_map_list\n",
    "# df_docStrList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Excel sheets\n",
    "pod_flavor_df = pd.read_csv(\"pod_flavors_25A_25B_EU_US.csv\")\n",
    "\n",
    "pf_docStrList = []\n",
    "pf_map_list = []\n",
    "\n",
    "for idx, row in pod_flavor_df.iterrows():\n",
    "    map={}\n",
    "    pod_type = row['Pod type']\n",
    "    pod_flavor = row['Pod flavor']\n",
    "    vCPU_req = row['vCPU Request (vCore)']\n",
    "    vCPU_limit = row['vCPU Limit (vCore)']\n",
    "    vMemory = row['vMemory (GB)']\n",
    "    hugepage = row['Hugepage (GB)']\n",
    "    per_vol = row['Persistent Volume (GB)']\n",
    "    map.update({'Pod type': pod_type, 'Pod flavor': pod_flavor, 'vCPU Request (vCore)': vCPU_req, 'vCPU Limit (vCore)': vCPU_limit, 'vMemory (GB)': vMemory, 'Hugepage (GB)': hugepage, 'Persistent Volume (GB)': per_vol})\n",
    "    pf_map_list.append(map)\n",
    "    content = f\"Pod type: {pod_type}\\n\"\n",
    "    content += f\"Pod flavor: {pod_flavor}\\n\"\n",
    "    content += f\"vCPU Request (vCore): {vCPU_req}\\n\"\n",
    "    content += f\"vCPU Limit (vCore): {vCPU_limit}\\n\"\n",
    "    content += f\"vMemory (GB): {vMemory}\\n\"\n",
    "    content += f\"Hugepage (GB): {hugepage}\\n\"\n",
    "    content += f\"Persistent Volume (GB): {per_vol}\\n\"\n",
    "\n",
    "    pf_docStrList.append(content.strip())\n",
    "\n",
    "# pf_docStrList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfDocList = [Document(page_content = doc) for doc in df_docStrList]\n",
    "pfDocList = [Document(page_content = doc) for doc in pf_docStrList]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the pre-trained model you want to use\n",
    "modelPath = \"all-MiniLM-L12-v2/\"\n",
    "\n",
    "# Create a dictionary with model configuration options, specifying to use the CPU for computations\n",
    "model_kwargs = {'device':'cpu'}\n",
    "\n",
    "# Create a dictionary with encoding options, specifically setting 'normalize_embeddings' to False\n",
    "encode_kwargs = {'normalize_embeddings': False}\n",
    "\n",
    "# Initialize an instance of HuggingFaceEmbeddings with the specified parameters\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=modelPath,     # Provide the pre-trained model's path\n",
    "    model_kwargs=model_kwargs, # Pass the model configuration options\n",
    "    encode_kwargs=encode_kwargs # Pass the encoding options\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfdb = FAISS.from_documents(dfDocList, embeddings)\n",
    "pfdb = FAISS.from_documents(pfDocList, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfdb.save_local(\"dfvectordb.bin\")\n",
    "pfdb.save_local(\"pfvectordb.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfdbIndex = FAISS.load_local(\"dfvectordb.bin\", embeddings, allow_dangerous_deserialization=True)\n",
    "pfdbIndex = FAISS.load_local(\"pfvectordb.bin\", embeddings, allow_dangerous_deserialization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = \"fdd-120m-12c-gsm-8trx-spr\"\n",
    "# testdoc = dfdbIndex.similarity_search(query, k = 4)\n",
    "# # print(testdoc[0].page_content)\n",
    "# for i in testdoc:\n",
    "#     print(i.page_content)\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query1 = \"fdd-270m-18c-gsm-6trx-spr\"\n",
    "# query2 = \"medium-uni\"\n",
    "# testdoc = pfdbIndex.similarity_search(query2, k = 3)\n",
    "# for i in testdoc:\n",
    "#     print(i.page_content)\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lexical and Semantic Search Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess the LLM output to extract relevant information in a structured format. This function assumes that the LLM output contains specific phrases indicating the dimensioning flavor, network function, and pod details. It extracts these details and formats them into a dictionary. The function uses regular expressions to identify and extract the required information from the LLM output. The extracted information includes the dimensioning flavor, network function, and a list of pods along with their respective flavors. The function returns a dictionary containing these details.\n",
    "import re\n",
    "def preprocess_df_data(llm_output: str):\n",
    "    dimensioning_flavor = \"Not Available\"\n",
    "    network_function = \"Not Available\"\n",
    "    pods = []\n",
    "\n",
    "    lines = llm_output.strip().splitlines()\n",
    "\n",
    "    for line in lines:\n",
    "        # Match dimensioning flavor\n",
    "        if re.search(r'Dimensioning Flavo[u]?r', line, re.IGNORECASE):\n",
    "            match = re.search(r'Dimensioning Flavo[u]?r\\s*[:\\-]\\s*(.+)', line, re.IGNORECASE)\n",
    "            if match:\n",
    "                dimensioning_flavor = match.group(1).strip()\n",
    "\n",
    "        # Match network function\n",
    "        elif re.search(r'Network Function', line, re.IGNORECASE):\n",
    "            match = re.search(r'Network Function\\s*[:\\-]\\s*(.+)', line, re.IGNORECASE)\n",
    "            if match:\n",
    "                network_function = match.group(1).strip()\n",
    "\n",
    "        # Match pods - Fixed regex pattern\n",
    "        else:\n",
    "            # Look for lines with pattern: - PodName: value\n",
    "            match = re.match(r'\\s*-\\s*([A-Za-z]{2,4}):\\s*(.+)', line.strip())\n",
    "            if match:\n",
    "                pod_name = match.group(1).strip()\n",
    "                pod_flavor = match.group(2).strip()\n",
    "                \n",
    "                # Only add if it looks like a pod (contains 'p' and isn't \"Package\")\n",
    "                if 'p' in pod_name.lower() and pod_name.lower() != 'package':\n",
    "                    pods.append({\n",
    "                        'pod_name': pod_name,\n",
    "                        'pod_flavor': pod_flavor\n",
    "                    })\n",
    "\n",
    "    return {\n",
    "        \"dimensioning_flavor\": dimensioning_flavor,\n",
    "        \"network_function\": network_function,\n",
    "        \"pods\": pods\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract dimensioning flavor information from a query using Semantic search. This function takes a query as input and performs a similarity search using the FAISS index to find the most relevant document. It then returns the retrieved document containing the dimensioning flavor information.\n",
    "def extract_dimension_flavor_info2(query: str):\n",
    "    dimension_flavor_data = \"\"\n",
    "    dimension_flavor_data = dfdbIndex.similarity_search(query,k=1)\n",
    "    return dimension_flavor_data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to convert a list of dictionaries to a context string suitable for LLM consumption. This function takes a list of dictionaries and a title as input. It iterates over the dictionaries, formats the key-value pairs, and constructs a context string with the formatted information. The context string includes a title and each dictionary's key-value pairs in a structured format. The function returns the constructed context string.\n",
    "def dict_to_context(data_dict_list, title=\"Context Information\"):\n",
    "    \"\"\"Convert dictionary to LLM-friendly context format with simple bullet points\"\"\"\n",
    "    context_lines = [f\"## {title}\\n\"]\n",
    "    \n",
    "    for i, data_dict in enumerate(data_dict_list, 1):\n",
    "        context_lines.append(f\"### Item {i}\")\n",
    "        for key, value in data_dict.items():\n",
    "            # Format key nicely (replace underscores, capitalize)\n",
    "            formatted_key = key.replace('_', ' ').title()\n",
    "            context_lines.append(f\"- {formatted_key}: {value}\")\n",
    "        context_lines.append(\"\")  # Add blank line between items\n",
    "    \n",
    "    return \"\\n\".join(context_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "FINAL PRODUCTION FIELD PARSER\n",
    "============================\n",
    "\n",
    "✅ Handles spelling mistakes (operatr -> operator, flavour -> flavor)\n",
    "✅ Multiple values per field  \n",
    "✅ Context separation (ignores explanatory text)\n",
    "✅ Complex query structures\n",
    "✅ Fuzzy field matching with edit distance\n",
    "✅ False positive filtering\n",
    "\n",
    "This is your ready-to-use production function.\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "from typing import Dict, Set, List, Optil\n",
    "\n",
    "def levenshtein_distance(s1: str, s2: str) -> int:\n",
    "    \"\"\"Calculate the Levenshtein edit distance between two strings.\"\"\"\n",
    "    if len(s1) < len(s2):\n",
    "        return levenshtein_distance(s2, s1)\n",
    "    if len(s2) == 0:\n",
    "        return len(s1)\n",
    "\n",
    "    previous_row = list(range(len(s2) + 1))\n",
    "    for i, c1 in enumerate(s1):\n",
    "        current_row = [i + 1]\n",
    "        for j, c2 in enumerate(s2):\n",
    "            insertions = previous_row[j + 1] + 1\n",
    "            deletions = current_row[j] + 1\n",
    "            substitutions = previous_row[j] + (c1 != c2)\n",
    "            current_row.append(min(insertions, deletions, substitutions))\n",
    "        previous_row = current_row\n",
    "    \n",
    "    return previous_row[-1]\n",
    "\n",
    "def fuzzy_match_score(candidate: str, target: str) -> float:\n",
    "    \"\"\"\n",
    "    Calculate fuzzy match score between candidate and target field names.\n",
    "    Handles spelling mistakes using edit distance and word similarity.\n",
    "    \"\"\"\n",
    "    candidate_lower = candidate.lower().strip()\n",
    "    target_lower = target.lower().strip()\n",
    "    \n",
    "    # Exact match\n",
    "    if candidate_lower == target_lower:\n",
    "        return 1.0\n",
    "    \n",
    "    # Handle British vs American spellings\n",
    "    candidate_norm = candidate_lower.replace('flavour', 'flavor').replace('colour', 'color')\n",
    "    target_norm = target_lower.replace('flavour', 'flavor').replace('colour', 'color')\n",
    "    \n",
    "    if candidate_norm == target_norm:\n",
    "        return 0.95\n",
    "    \n",
    "    # Calculate edit distance similarity\n",
    "    distance = levenshtein_distance(candidate_lower, target_lower)\n",
    "    max_len = max(len(candidate_lower), len(target_lower))\n",
    "    \n",
    "    if max_len == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    similarity = 1.0 - (distance / max_len)\n",
    "    \n",
    "    # Word-based matching for compound fields\n",
    "    candidate_words = set(candidate_lower.split())\n",
    "    target_words = set(target_lower.split())\n",
    "    \n",
    "    if candidate_words and target_words:\n",
    "        word_overlap = len(candidate_words.intersection(target_words))\n",
    "        total_words = len(candidate_words.union(target_words))\n",
    "        word_similarity = word_overlap / total_words if total_words > 0 else 0\n",
    "        similarity = 0.6 * similarity + 0.4 * word_similarity\n",
    "    \n",
    "    return similarity\n",
    "\n",
    "def clean_query(query: str) -> str:\n",
    "    \"\"\"Clean query by removing extra characters that interfere with parsing.\"\"\"\n",
    "    query = re.sub(r'\"{2,}', '\"', query)  # Remove multiple quotes\n",
    "    query = re.sub(r'[(){}\\[\\]]', '', query)  # Remove brackets\n",
    "    query = re.sub(r'\\s+', ' ', query)  # Normalize spaces\n",
    "    return query.strip()\n",
    "\n",
    "def separate_context_from_query(query: str) -> str:\n",
    "    \"\"\"Separate main query from contextual/explanatory information.\"\"\"\n",
    "    context_markers = [\n",
    "        r'just\\s+for\\s+the\\s+context',\n",
    "        r'these\\s+all\\s+are',\n",
    "        r'also\\s+called\\s+as',\n",
    "        r'strings\\s+like'\n",
    "    ]\n",
    "    \n",
    "    earliest_pos = len(query)\n",
    "    for marker in context_markers:\n",
    "        match = re.search(marker, query, re.IGNORECASE)\n",
    "        if match and match.start() < earliest_pos:\n",
    "            earliest_pos = match.start()\n",
    "    \n",
    "    return query[:earliest_pos].strip() if earliest_pos < len(query) else query\n",
    "\n",
    "def extract_field_value_assignments(query: str) -> List[tuple]:\n",
    "    \"\"\"\n",
    "    Extract field-value assignments using character-by-character scanning.\n",
    "    This approach reliably finds field=value and field:\"value\" patterns.\n",
    "    \"\"\"\n",
    "    pairs = []\n",
    "    \n",
    "    i = 0\n",
    "    while i < len(query):\n",
    "        if query[i] in '=:':\n",
    "            # Found assignment operator\n",
    "            \n",
    "            # Look backwards for field name\n",
    "            field_start = i - 1\n",
    "            while field_start >= 0 and query[field_start].isspace():\n",
    "                field_start -= 1  # Skip whitespace\n",
    "            \n",
    "            if field_start >= 0:\n",
    "                # Find start of field name (go back to word boundary)\n",
    "                field_end = field_start + 1\n",
    "                while field_start >= 0 and (query[field_start].isalnum() or query[field_start] in ' _-'):\n",
    "                    field_start -= 1\n",
    "                field_start += 1\n",
    "                \n",
    "                field_candidate = query[field_start:field_end].strip()\n",
    "                \n",
    "                # Look forwards for value\n",
    "                value_start = i + 1\n",
    "                while value_start < len(query) and query[value_start].isspace():\n",
    "                    value_start += 1  # Skip whitespace\n",
    "                \n",
    "                if value_start < len(query):\n",
    "                    # Handle quoted values\n",
    "                    if query[value_start] in '\"\\'':\n",
    "                        quote_char = query[value_start]\n",
    "                        value_end = value_start + 1\n",
    "                        while value_end < len(query) and query[value_end] != quote_char:\n",
    "                            value_end += 1\n",
    "                        if value_end < len(query):\n",
    "                            value = query[value_start + 1:value_end]\n",
    "                            pairs.append((field_candidate, value))\n",
    "                            i = value_end + 1\n",
    "                            continue\n",
    "                    else:\n",
    "                        # Handle unquoted values\n",
    "                        value_end = value_start\n",
    "                        while (value_end < len(query) and \n",
    "                               query[value_end] not in ' ,\\n\\r\\t' and\n",
    "                               not (value_end < len(query) - 3 and query[value_end:value_end+4].lower() == ' and')):\n",
    "                            value_end += 1\n",
    "                        \n",
    "                        value = query[value_start:value_end].strip()\n",
    "                        if (len(value) >= 2 and \n",
    "                            value.lower() not in {'and', 'or', 'the', 'for', 'with'}):\n",
    "                            pairs.append((field_candidate, value))\n",
    "                        \n",
    "                        i = value_end\n",
    "                        continue\n",
    "        i += 1\n",
    "    \n",
    "    # Clean and validate field candidates\n",
    "    cleaned_pairs = []\n",
    "    for field_candidate, value in pairs:\n",
    "        # Remove common words from field candidate\n",
    "        words = field_candidate.split()\n",
    "        filtered_words = []\n",
    "        for word in words:\n",
    "            if word.lower() not in {'for', 'the', 'and', 'or', 'extract', 'information', 'following'}:\n",
    "                filtered_words.append(word)\n",
    "        \n",
    "        if filtered_words:\n",
    "            clean_field = ' '.join(filtered_words)\n",
    "            \n",
    "            # Additional validation to avoid false positives\n",
    "            if (len(clean_field.split()) <= 3 and  # Max 3 words for field names\n",
    "                len(value) >= 2 and\n",
    "                not value.startswith('<') and  # Avoid template placeholders\n",
    "                not re.match(r'^[0-9]+\\.$', value)):  # Avoid numbered lists\n",
    "                cleaned_pairs.append((clean_field, value))\n",
    "    \n",
    "    return cleaned_pairs\n",
    "\n",
    "def find_best_field_match_fuzzy(candidate_field: str, available_fields: Set[str]) -> Optional[str]:\n",
    "    \"\"\"Find best matching available field using fuzzy matching with edit distance.\"\"\"\n",
    "    best_match = None\n",
    "    best_score = 0.0\n",
    "    min_score = 0.5  # Threshold for accepting matches\n",
    "    \n",
    "    for available_field in available_fields:\n",
    "        score = fuzzy_match_score(candidate_field, available_field)\n",
    "        if score > best_score and score >= min_score:\n",
    "            best_score = score\n",
    "            best_match = available_field\n",
    "    \n",
    "    return best_match\n",
    "\n",
    "def parse_query_for_fields(query: str, available_fields: Set[str]) -> Dict[str, List[str]]:\n",
    "    \"\"\"\n",
    "    Parse natural language query to extract field-value pairs.\n",
    "    \n",
    "    PRODUCTION-READY FEATURES:\n",
    "    ✅ Spelling mistake tolerance (uses edit distance)\n",
    "    ✅ Multiple values per field support\n",
    "    ✅ Context separation (ignores explanatory text)\n",
    "    ✅ Handles complex query structures\n",
    "    ✅ Fuzzy field name matching\n",
    "    ✅ False positive filtering\n",
    "    \n",
    "    Args:\n",
    "        query (str): Natural language query (may contain spelling mistakes)\n",
    "        available_fields (Set[str]): Set of exact available field names\n",
    "    \n",
    "    Returns:\n",
    "        Dict[str, List[str]]: Extracted field-value pairs with multiple values support\n",
    "        \n",
    "    Examples:\n",
    "        >>> fields = {'Operator', 'Dimensioning Flavor'}\n",
    "        >>> parse_query_for_fields(\"operatr=VOS and dimensioning flavour=large\", fields)\n",
    "        {'Operator': ['VOS'], 'Dimensioning Flavor': ['large']}\n",
    "    \"\"\"\n",
    "    # Step 1: Clean and prepare query\n",
    "    query_cleaned = clean_query(query)\n",
    "    main_query = separate_context_from_query(query_cleaned)\n",
    "    \n",
    "    # Step 2: Extract field-value assignments\n",
    "    field_value_pairs = extract_field_value_assignments(main_query)\n",
    "    \n",
    "    # Step 3: Map to available fields using fuzzy matching\n",
    "    field_criteria = {}\n",
    "    \n",
    "    for field_candidate, value in field_value_pairs:\n",
    "        matched_field = find_best_field_match_fuzzy(field_candidate, available_fields)\n",
    "        \n",
    "        if matched_field:\n",
    "            if matched_field in field_criteria:\n",
    "                if value not in field_criteria[matched_field]:\n",
    "                    field_criteria[matched_field].append(value)\n",
    "            else:\n",
    "                field_criteria[matched_field] = [value]\n",
    "    \n",
    "    return field_criteria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract dimension flavor info from query using Lexical search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to extract dimension flavor amd pod flavor info from query using Lexical search\n",
    "\n",
    "\n",
    "def extract_documents_from_query(documents: List[Dict], query: str) -> List[Any]:\n",
    "    \"\"\"\n",
    "    Extract documents based on a natural language query by automatically\n",
    "    identifying field-value pairs in the query.\n",
    "    \n",
    "    Returns:\n",
    "        List: List of matching Document objects\n",
    "    \"\"\"\n",
    "    # First, get all available fields from the documents\n",
    "    available_fields = documents[0].keys() if documents else set()\n",
    "    \n",
    "    # Parse the query to extract field-value pairs\n",
    "    # field_criteria = parse_query_for_fields(query, available_fields)\n",
    "    field_criteria =parse_query_for_fields(query,available_fields) \n",
    "    \n",
    "    \n",
    "    if not field_criteria:\n",
    "        print(\"No valid field-value pairs found in the query.\")\n",
    "        return []\n",
    "    \n",
    "    #print(f\"Extracted criteria from query: {field_criteria}\")\n",
    "    \n",
    "    # Find matching documents\n",
    "    return find_matching_documents(documents, field_criteria)\n",
    "\n",
    "def get_all_fields_from_documents(document: Any) -> set:\n",
    "    \"\"\"\n",
    "    Extract all unique field names from all documents.\n",
    "    \n",
    "    Args:\n",
    "        documents (List): List of Document objects\n",
    "    \n",
    "    Returns:\n",
    "        set: Set of all unique field names\n",
    "    \"\"\"\n",
    "    all_fields = set()\n",
    "    \n",
    "    lines = document.page_content.strip().split('\\n')\n",
    "    for line in lines:\n",
    "        if ':' in line:\n",
    "            field_name = line.split(':', 1)[0].strip()\n",
    "            all_fields.add(field_name)\n",
    "    \n",
    "    return all_fields\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def find_closest_field_match(query_field: str, available_fields: set) -> str:\n",
    "#     \"\"\"\n",
    "#     Find the closest matching field name from available fields.\n",
    "    \n",
    "#     Args:\n",
    "#         query_field (str): Field name from query\n",
    "#         available_fields (set): Available field names\n",
    "    \n",
    "#     Returns:\n",
    "#         str: Closest matching field name or None\n",
    "#     \"\"\"\n",
    "#     query_field_lower = query_field.lower().strip()\n",
    "    \n",
    "#     # Exact match\n",
    "#     for field in available_fields:\n",
    "#         if field.lower() == query_field_lower:\n",
    "#             return field\n",
    "    \n",
    "#     # Partial match\n",
    "#     for field in available_fields:\n",
    "#         if query_field_lower in field.lower() or field.lower() in query_field_lower:\n",
    "#             return field\n",
    "    \n",
    "#     # Keywords mapping for common variations\n",
    "#     field_keywords = {\n",
    "#         'dimension': 'Dimensioning Flavour',\n",
    "#         'dimensioning': 'Dimensioning Flavour',\n",
    "#         'flavour': 'Dimensioning Flavour',\n",
    "#         'flavor': 'Dimensioning Flavour',\n",
    "#         'operator': 'Operator',\n",
    "#         'network': 'Network Function',\n",
    "#         'function': 'Network Function',\n",
    "#         'package': 'Package',\n",
    "#     }\n",
    "    \n",
    "#     for keyword, field_name in field_keywords.items():\n",
    "#         if keyword in query_field_lower and field_name in available_fields:\n",
    "#             return field_name\n",
    "    \n",
    "#     return None\n",
    "\n",
    "def find_matching_documents(documents: List[Dict], field_criteria: Dict[str, str]) -> List[Any]:\n",
    "    \"\"\"\n",
    "    Find documents that match the given field criteria.\n",
    "    \n",
    "    Args:\n",
    "        documents (List): List of Document objects\n",
    "        field_criteria (Dict): Field-value pairs to match\n",
    "    \n",
    "    Returns:\n",
    "        List: List of matching Document objects\n",
    "    \"\"\"\n",
    "    match_documents= []\n",
    "\n",
    "    for doc in documents:  \n",
    "        isMatch = True  \n",
    "        for field_name, field_values in field_criteria.items():  \n",
    "            if field_name not in doc:  \n",
    "                isMatch = False  \n",
    "                break  \n",
    "            match_found = False  \n",
    "            for value in field_values:  \n",
    "                if str(doc[field_name]).lower() == str(value).lower():  \n",
    "                    match_found = True  \n",
    "                    break  # Exit inner loop on first match  \n",
    "            if not match_found:  \n",
    "                isMatch = False  \n",
    "                break  # Exit field loop if no match  \n",
    "        if isMatch:  \n",
    "            match_documents.append(doc)  \n",
    "    \n",
    "    return match_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to extract pod flavor info from query using lexical search\n",
    "def extract_pod_flavor_info(df_result: dict):\n",
    "    relData = \"\"\n",
    "    res=[]\n",
    "    for i in df_result['pods']:\n",
    "        query ='Pod type' + \"=\" + i['pod_name'] + ',' + \"Pod flavor\" +\"=\" + i['pod_flavor']\n",
    "        #print(query)\n",
    "        extracted_documents = extract_documents_from_query(pf_map_list,query)\n",
    "        res+=extracted_documents\n",
    "            # res = pfdbIndex.similarity_search(query)\n",
    "        # print(res)\n",
    "\n",
    "    relData = relData + dict_to_context(res)  + \"\\n\"\n",
    "    \n",
    "    print(relData)\n",
    "    return relData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_result2= {'dimensioning_flavor': 'medium-regular-sn-t20', 'network_function': 'uADPF', 'pods': [{'pod_name': 'DIP', 'pod_flavor': 'medium-2m'}, {'pod_name': 'DMP', 'pod_flavor': 'medium-uni'}, {'pod_name': 'CMP', 'pod_flavor': 'medium-uni'}, {'pod_name': 'PMP', 'pod_flavor': 'medium-uni'}, {'pod_name': 'RMP', 'pod_flavor': 'medium-uni'}, {'pod_name': 'IPP', 'pod_flavor': 'none.'}, {'pod_name': 'DPP', 'pod_flavor': 'medium-regular-sn-t20-xaca'}]}\n",
    "# extract_pod_flavor_info(df_result2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to route the query to appropriate database based on the query content. This function takes a query as input and checks for specific keywords to determine whether to route it to the dimensioning flavor database or the pod flavor database. If the query contains keywords related to dimensioning, it routes to the dimensioning flavor database. Otherwise, it routes to the pod flavor database. The function returns the appropriate FAISS index object for further processing.\n",
    "def route_query(query: str):\n",
    "    # Define some simple rules for routing based on keywords\n",
    "    dfdb_keywords = [\"dimensioning\"]\n",
    "    pfdb_keywords = [\"resources\"]\n",
    "\n",
    "    query = query.lower()\n",
    "\n",
    "    # Check for keywords and route to corresponding DB\n",
    "    if any(keyword in query for keyword in dfdb_keywords):\n",
    "        return dfdbIndex\n",
    "    elif any(keyword in query for keyword in pfdb_keywords):\n",
    "        return pfdbIndex\n",
    "    else:\n",
    "        # Default or fall-back routing\n",
    "        return dfdbIndex  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count token in prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(r\"C:\\Users\\aakash.a1\\Documents\\VRN\\Qwen3-32B\")\n",
    "\n",
    "def num_tokens(text):\n",
    "    tokens = tokenizer.encode(text)\n",
    "    # print(\"Token count:\", len(tokens))\n",
    "    return len(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a trimmed prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(r\"C:\\Users\\aakash.a1\\Documents\\VRN\\Qwen3-32B\")\n",
    "\n",
    "MAX_CONTEXT_TOKENS = 32000         # Update to match your LLM\n",
    "RESERVED_FOR_RESPONSE = 1024       # Tokens reserved for model's generated answer\n",
    "\n",
    "def num_token(text):\n",
    "    return len(tokenizer.encode(text))\n",
    "\n",
    "def trim_history(qa_history, max_tokens=MAX_CONTEXT_TOKENS-RESERVED_FOR_RESPONSE):\n",
    "    \"\"\"\n",
    "    Keeps as many recent history turns (Q&A) as fit in max_tokens.\n",
    "    \"\"\"\n",
    "    trimmed = []\n",
    "    total = 0\n",
    "    # Only add from the *end* (most recent) and prepend for right order\n",
    "    for q, a in reversed(qa_history):\n",
    "        block = f\"Q: {q}\\nA: {a}\"\n",
    "        t = num_tokens(block)\n",
    "        if total + t > max_tokens:\n",
    "            break\n",
    "        trimmed.insert(0, (q, a))\n",
    "        total += t\n",
    "    return trimmed\n",
    "\n",
    "def summarize_if_needed(answer, max_tokens=512):\n",
    "    # If response is too long, just truncate. Could be replaced by an LLM-driven summary\n",
    "    tokens = tokenizer.encode(answer)\n",
    "    if len(tokens) > max_tokens:\n",
    "        tokens = tokens[:max_tokens]\n",
    "        return tokenizer.decode(tokens)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TOKENS = 32000  # use correct value for your model\n",
    "RESERVED_TOKENS = 1024  # for the model's output\n",
    "\n",
    "def build_prompt(system_prompt, history, rag_context, user_question):\n",
    "    # Start with the new question and RAG context\n",
    "    prompt = f\"{system_prompt}\\nContext:\\n{rag_context}\\nUser: {user_question}\\n\"\n",
    "    current_tokens = num_tokens(prompt)\n",
    "    \n",
    "    # Add history, starting from the most recent\n",
    "    for q, a in reversed(history):  # history = list of (Q, A) pairs\n",
    "        block = f\"User: {q}\\nAssistant: {a}\\n\"\n",
    "        if current_tokens + num_tokens(block) > MAX_TOKENS - RESERVED_TOKENS:\n",
    "            break  # Stop adding if you would exceed the limit\n",
    "        prompt = block + prompt  # Add to the front for chronological order\n",
    "        current_tokens += num_tokens(block)\n",
    "\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in c:\\users\\aakash.a1\\appdata\\local\\anaconda3\\envs\\rag-env\\lib\\site-packages (0.3.26)\n",
      "Requirement already satisfied: ollama in c:\\users\\aakash.a1\\appdata\\local\\anaconda3\\envs\\rag-env\\lib\\site-packages (0.5.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\aakash.a1\\appdata\\local\\anaconda3\\envs\\rag-env\\lib\\site-packages (0.4.6)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.66 in c:\\users\\aakash.a1\\appdata\\local\\anaconda3\\envs\\rag-env\\lib\\site-packages (from langchain) (0.3.66)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in c:\\users\\aakash.a1\\appdata\\local\\anaconda3\\envs\\rag-env\\lib\\site-packages (from langchain) (0.3.8)\n",
      "Requirement already satisfied: langsmith>=0.1.17 in c:\\users\\aakash.a1\\appdata\\local\\anaconda3\\envs\\rag-env\\lib\\site-packages (from langchain) (0.4.3)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\aakash.a1\\appdata\\local\\anaconda3\\envs\\rag-env\\lib\\site-packages (from langchain) (2.11.7)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\aakash.a1\\appdata\\local\\anaconda3\\envs\\rag-env\\lib\\site-packages (from langchain) (2.0.41)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\aakash.a1\\appdata\\local\\anaconda3\\envs\\rag-env\\lib\\site-packages (from langchain) (2.32.4)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\aakash.a1\\appdata\\local\\anaconda3\\envs\\rag-env\\lib\\site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in c:\\users\\aakash.a1\\appdata\\local\\anaconda3\\envs\\rag-env\\lib\\site-packages (from langchain) (4.0.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\aakash.a1\\appdata\\local\\anaconda3\\envs\\rag-env\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\aakash.a1\\appdata\\local\\anaconda3\\envs\\rag-env\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\aakash.a1\\appdata\\local\\anaconda3\\envs\\rag-env\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\aakash.a1\\appdata\\local\\anaconda3\\envs\\rag-env\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (4.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\aakash.a1\\appdata\\local\\anaconda3\\envs\\rag-env\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain) (3.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\aakash.a1\\appdata\\local\\anaconda3\\envs\\rag-env\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\aakash.a1\\appdata\\local\\anaconda3\\envs\\rag-env\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\aakash.a1\\appdata\\local\\anaconda3\\envs\\rag-env\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\aakash.a1\\appdata\\local\\anaconda3\\envs\\rag-env\\lib\\site-packages (from requests<3,>=2->langchain) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\aakash.a1\\appdata\\local\\anaconda3\\envs\\rag-env\\lib\\site-packages (from requests<3,>=2->langchain) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\aakash.a1\\appdata\\local\\anaconda3\\envs\\rag-env\\lib\\site-packages (from requests<3,>=2->langchain) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\aakash.a1\\appdata\\local\\anaconda3\\envs\\rag-env\\lib\\site-packages (from requests<3,>=2->langchain) (2025.6.15)\n",
      "Requirement already satisfied: greenlet>=1 in c:\\users\\aakash.a1\\appdata\\local\\anaconda3\\envs\\rag-env\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.3)\n",
      "Requirement already satisfied: httpx>=0.27 in c:\\users\\aakash.a1\\appdata\\local\\anaconda3\\envs\\rag-env\\lib\\site-packages (from ollama) (0.28.1)\n",
      "Requirement already satisfied: anyio in c:\\users\\aakash.a1\\appdata\\local\\anaconda3\\envs\\rag-env\\lib\\site-packages (from httpx>=0.27->ollama) (4.9.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\aakash.a1\\appdata\\local\\anaconda3\\envs\\rag-env\\lib\\site-packages (from httpx>=0.27->ollama) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\aakash.a1\\appdata\\local\\anaconda3\\envs\\rag-env\\lib\\site-packages (from httpcore==1.*->httpx>=0.27->ollama) (0.16.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\aakash.a1\\appdata\\local\\anaconda3\\envs\\rag-env\\lib\\site-packages (from langsmith>=0.1.17->langchain) (3.10.18)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\aakash.a1\\appdata\\local\\anaconda3\\envs\\rag-env\\lib\\site-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\users\\aakash.a1\\appdata\\local\\anaconda3\\envs\\rag-env\\lib\\site-packages (from langsmith>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\aakash.a1\\appdata\\local\\anaconda3\\envs\\rag-env\\lib\\site-packages (from anyio->httpx>=0.27->ollama) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\aakash.a1\\appdata\\local\\anaconda3\\envs\\rag-env\\lib\\site-packages (from anyio->httpx>=0.27->ollama) (1.3.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain ollama colorama "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GAP classs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.language_models.llms import LLM\n",
    "from typing import Optional, List\n",
    "import requests\n",
    "import sys\n",
    "import time\n",
    "\n",
    "class GapLLM(LLM):\n",
    "    model_name: str = \"Qwen/Qwen3-32B\"\n",
    "    temperature: float = 0.1\n",
    "    max_tokens: int = 16384\n",
    "    top_p: float = 0.8\n",
    "    top_k: int = 40\n",
    "    repetition_penalty: float = 1.18\n",
    "    key: str = \"50n6kyhbspf7nrywpmjbif5bnb74xy9e\"\n",
    "    api_url: str = \"https://api.gap-srib.com/gap/v2/txt/txt\"\n",
    "    app_id: str = \"srOZp3TxPOn=\"\n",
    "\n",
    "    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n",
    "        current_time = str(int(time.time() * 1000))\n",
    "        headers = {\n",
    "            \"Content-Type\": \"application/json\",\n",
    "            \"Authorization\": f\"Bearer {self.key}\"\n",
    "        }\n",
    "        body = {\n",
    "            \"app_id\": self.app_id,\n",
    "            \"mode\": \"chat\",\n",
    "            \"max_tokens\": self.max_tokens,\n",
    "            \"temperature\": self.temperature, \n",
    "            \"top_p\": self.top_p,\n",
    "            \"top_k\": self.top_k,\n",
    "            \"typical_p\": 0.6,\n",
    "            \"repetition_penalty\": self.repetition_penalty,\n",
    "            \"task_id\": current_time,\n",
    "            # \"stream\":True,\n",
    "            \"prompt\": prompt,\n",
    "            \"preferred_model\": self.model_name\n",
    "        }\n",
    "\n",
    "        res = requests.post(self.api_url, headers=headers, json=body, verify=False)\n",
    "        res.raise_for_status()\n",
    "\n",
    "        data = res.json()\n",
    "\n",
    "        # Extract from `model_response` list safely\n",
    "        if isinstance(data.get(\"model_response\"), list) and data[\"model_response\"]:\n",
    "            return data[\"model_response\"][0].strip()\n",
    "        else:\n",
    "            return \"[Error] No valid response received from model.\"\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"gap-custom-llm\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PROMPTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1=\"You are an assistant. Based on the following context:\\n{retrieved_context}\\n\\nChat History:\\n{chat_history}\\n\\nAnswer this:\\n{question}\"\n",
    "prompt2=\"You are a concise assistant. If the user greets you, respond politely in one sentence.\\nUser: Hi\\nAssistant:\"\n",
    "prompt3= \"You are an assistant. Based on the following context:\\n{retrieved_context}\\n\\nChat History:\\n{chat_history}\\n\\nIMPORTANT: Only answer the specific question asked. Do not invent conversations or add questions that weren't asked\\n\\nDo not invent questions\\n\\nAnswer this question only:\\n{question}\"\n",
    "prompt4= \"You are a helpful assistant, who can generate the answer for the given Question.\\nIf you don't know the answer, say I don't know.\\nDon't try to make up an answer.\\nMake sure to answer in crisp manner.\\nDo Not hallucinate.\\nAnswer in English language only.\\nDo not repeat the question. \\nIf there are multiple answers, give all of them.\\nIf you need more information from user, ask them to be more specific.\\nDo not give confusing answers.\\nIf you're not sure, say I don't know.\\nAlways assist with care, respect, and truth. Respond with utmost utility yet securely. Avoid harmful, unethical, prejudiced, or negative content. Ensure replies promote fairness and positivity.\"\n",
    "prompt5=\"<s>\\n[INST]\\nYou are a helpful assistant, who can generate the answer for the given Question.\\nIf you don't know the answer, say I don't know.\\nDon't try to make up an answer.\\nMake sure to answer in crisp manner.\\nDo Not hallucinate.\\nAnswer in English language only.\\nDo not repeat the question. \\nIf there are multiple answers, give all of them.\\nIf you need more information from user, ask them to be more specific.\\nDo not give confusing answers.\\nIf you're not sure, say I don't know.\\nAlways assist with care, respect, and truth. Respond with utmost utility yet securely. Avoid harmful, unethical, prejudiced, or negative content. Ensure replies promote fairness and positivity.\\n[/INST]\\n</s>\\n[INST]\\n{retrieved_context}\\n[/INST]\\n\\n[INST]\\n{chat_history}\\n[/INST]\\n[INST]\\nQuestion: {question}\\nAnswer: \\n[/INST]\\n[INST]\\nGive short and crisp answer.\\nDon't give additional information.\\n[/INST]\"\n",
    "prompt6= \"<s>\\n[INST]\\nYou are a helpful assistant, who can generate the answer for the given Question.\\nIf you don't know the answer, say I don't know.\\nDon't try to make up an answer.\\nMake sure to answer in crisp manner.\\nPlease keep going until the user’s query is completely resolved, before ending your turn and yielding back to the user. Only terminate your turn when you are sure that the problem is solved.\\n\\nDo Not hallucinate.\\nAnswer in English language only.\\nDo not repeat the question. \\nIf there are multiple answers, give all of them.\\nTake your time and go through each statement line by line before and after each action you decide to take.\\n\\nYou already have everything you need even without internet connection.\\n\\nIf you need more information from user, ask them to be more specific.\\nDo not give confusing answers.\\nIf you're not sure, say I don't know.\\nAlways assist with care, respect, and truth. Respond with utmost utility yet securely. Avoid harmful, unethical, prejudiced, or negative content. Ensure replies promote fairness and positivity.\\n[/INST]\\n</s>\\n[INST]\\n{retrieved_context}\\n[/INST]\\n\\n[INST]\\n{chat_history}\\n[/INST]\\n[INST]\\nQuestion: {question}\\nAnswer: \\n[/INST]\\n[INST]\\nGive short and crisp answer.\\nDon't give additional information.\\n[/INST]\"\n",
    "prompt7= \"<s>\\n[INST]\\nYou are a Network pod placement assistant, who can generate the answer for the given Question.\\nIf you don't know the answer, say I don't know.\\nDon't try to make up an answer.\\nMake sure to answer in crisp manner.\\nPlease keep going until the user’s query is completely resolved, before ending your turn and yielding back to the user..\\n\\nDo Not hallucinate.\\nAnswer in English language only.\\nDo not repeat the question. \\nIf there are multiple answers, give all of them.\\nTake your time and go through each statement line by line before and after each action you decide to take.\\n\\nYou already have everything you need even without internet connection.\\n\\nIf you need more information from user, ask them to be more specific.\\nDo not give confusing answers.\\nDo not include any <Think></Think> tags or similar blocks in your response.Provide only the final answer, and do not show any <Think>…</Think> or reasoning blocks.\\nIf you're not sure, say I don't know.\\nAlways assist with care, respect, and truth. Respond with utmost utility yet securely. Avoid harmful, unethical, prejudiced, or negative content. Ensure replies promote fairness and positivity.\\n[/INST]\\n</s>\\n[INST]\\n{retrieved_context}\\n[/INST]\\n\\n[INST]\\n{chat_history}\\n[/INST]\\n[INST]\\nQuestion: {question}\\nAnswer: \\n[/INST]\\n[INST]\\nGive short and crisp answer.\\nDon't give additional information.\\n[/INST]\"\n",
    "prompt8= \"<s>\\n[INST]\\nYou are a Network pod placement assistant, who can generate the answer for the given Question.\\nIf you don't know the answer, say I don't know.\\nDon't try to make up an answer.\\nMake sure to answer in crisp manner.\\nPlease keep going until the user’s query is completely resolved, before ending your turn and yielding back to the user.\\n\\nDo Not hallucinate.\\nAnswer in English language only.\\nDo not repeat the question. \\nIf there are multiple answers, give all of them.\\nTake your time and go through each statement line by line.\\n\\nYou already have everything you need even without internet connection.\\n\\nIf you need more information from user, ask them to be more specific.\\nDo not give confusing answers.\\nIf you're not sure, say I don't know.\\nAlways assist with care, respect, and truth. Respond with utmost utility yet securely. Avoid harmful, unethical, prejudiced, or negative content. Ensure replies promote fairness and positivity.\\n[/INST]\\n</s>\\n[INST]\\n{retrieved_context}\\n[/INST]\\n\\n[INST]\\n{chat_history}\\n[/INST]\\n[INST]\\nQuestion: {question}\\nAnswer: \\n[/INST]\\n[INST]\\nGive short and crisp answer.\\nDon't give additional information.\\n[/INST]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GAP Qwen/Qwen3-32B WITH  most similar context using FAISS (assuming db_index is set up) LOGIC OPTIMIZED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 AI Assistant is ready! Type 'exit' to stop."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📚 Context Source: Dimensioning Database\n",
      "[{'Operator': 'VOS', 'Network Function': 'uADPF', 'Dimensioning Flavor': 'medium-regular-sn-t20', 'Package': '25B', 'DPP': 'medium-regular-sn-t20-xaca', 'DIP': 'medium-2m', 'DMP': 'medium-uni', 'CMP': 'medium-uni', 'PMP': 'medium-uni', 'RMP': 'medium-uni', 'IPP': nan}, {'Operator': 'VOS', 'Network Function': 'uADPF', 'Dimensioning Flavor': 'medium-regular-gnr-t20', 'Package': '25B', 'DPP': 'medium-regular-gsm-gnr-t20', 'DIP': 'medium-2m', 'DMP': 'medium-uni', 'CMP': 'medium-uni', 'PMP': 'medium-uni', 'RMP': 'medium-uni', 'IPP': 'small'}]\n",
      "## Context Information\n",
      "\n",
      "### Item 1\n",
      "- Operator: VOS\n",
      "- Network Function: uADPF\n",
      "- Dimensioning Flavor: medium-regular-sn-t20\n",
      "- Package: 25B\n",
      "- Dpp: medium-regular-sn-t20-xaca\n",
      "- Dip: medium-2m\n",
      "- Dmp: medium-uni\n",
      "- Cmp: medium-uni\n",
      "- Pmp: medium-uni\n",
      "- Rmp: medium-uni\n",
      "- Ipp: nan\n",
      "\n",
      "### Item 2\n",
      "- Operator: VOS\n",
      "- Network Function: uADPF\n",
      "- Dimensioning Flavor: medium-regular-gnr-t20\n",
      "- Package: 25B\n",
      "- Dpp: medium-regular-gsm-gnr-t20\n",
      "- Dip: medium-2m\n",
      "- Dmp: medium-uni\n",
      "- Cmp: medium-uni\n",
      "- Pmp: medium-uni\n",
      "- Rmp: medium-uni\n",
      "- Ipp: small\n",
      "\n",
      "[Debug] Input Tokens This Turn: 360\n",
      "[Debug] Total Input Tokens This Session: 360\n",
      "[Debug] Token usage per question: [360]\n",
      "df-results {'dimensioning_flavor': 'medium-regular-gnr-t20', 'network_function': 'uADPF', 'pods': [{'pod_name': 'Dpp', 'pod_flavor': 'medium-regular-sn-t20-xaca'}, {'pod_name': 'Dip', 'pod_flavor': 'medium-2m'}, {'pod_name': 'Dmp', 'pod_flavor': 'medium-uni'}, {'pod_name': 'Cmp', 'pod_flavor': 'medium-uni'}, {'pod_name': 'Pmp', 'pod_flavor': 'medium-uni'}, {'pod_name': 'Rmp', 'pod_flavor': 'medium-uni'}, {'pod_name': 'Ipp', 'pod_flavor': 'nan'}, {'pod_name': 'Dpp', 'pod_flavor': 'medium-regular-gsm-gnr-t20'}, {'pod_name': 'Dip', 'pod_flavor': 'medium-2m'}, {'pod_name': 'Dmp', 'pod_flavor': 'medium-uni'}, {'pod_name': 'Cmp', 'pod_flavor': 'medium-uni'}, {'pod_name': 'Pmp', 'pod_flavor': 'medium-uni'}, {'pod_name': 'Rmp', 'pod_flavor': 'medium-uni'}, {'pod_name': 'Ipp', 'pod_flavor': 'small'}]}\n",
      "\n",
      "💬 Answer:\n",
      "## Context Information\n",
      "\n",
      "### Item 1\n",
      "- Operator: VOS\n",
      "- Network Function: uADPF\n",
      "- Dimensioning Flavor: medium-regular-sn-t20\n",
      "- Package: 25B\n",
      "- Dpp: medium-regular-sn-t20-xaca\n",
      "- Dip: medium-2m\n",
      "- Dmp: medium-uni\n",
      "- Cmp: medium-uni\n",
      "- Pmp: medium-uni\n",
      "- Rmp: medium-uni\n",
      "- Ipp: nan\n",
      "\n",
      "### Item 2\n",
      "- Operator: VOS\n",
      "- Network Function: uADPF\n",
      "- Dimensioning Flavor: medium-regular-gnr-t20\n",
      "- Package: 25B\n",
      "- Dpp: medium-regular-gsm-gnr-t20\n",
      "- Dip: medium-2m\n",
      "- Dmp: medium-uni\n",
      "- Cmp: medium-uni\n",
      "- Pmp: medium-uni\n",
      "- Rmp: medium-uni\n",
      "- Ipp: small\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "🔍 Dimensioning Flavor Result: {'dimensioning_flavor': 'medium-regular-gnr-t20', 'network_function': 'uADPF', 'pods': [{'pod_name': 'Dpp', 'pod_flavor': 'medium-regular-sn-t20-xaca'}, {'pod_name': 'Dip', 'pod_flavor': 'medium-2m'}, {'pod_name': 'Dmp', 'pod_flavor': 'medium-uni'}, {'pod_name': 'Cmp', 'pod_flavor': 'medium-uni'}, {'pod_name': 'Pmp', 'pod_flavor': 'medium-uni'}, {'pod_name': 'Rmp', 'pod_flavor': 'medium-uni'}, {'pod_name': 'Ipp', 'pod_flavor': 'nan'}, {'pod_name': 'Dpp', 'pod_flavor': 'medium-regular-gsm-gnr-t20'}, {'pod_name': 'Dip', 'pod_flavor': 'medium-2m'}, {'pod_name': 'Dmp', 'pod_flavor': 'medium-uni'}, {'pod_name': 'Cmp', 'pod_flavor': 'medium-uni'}, {'pod_name': 'Pmp', 'pod_flavor': 'medium-uni'}, {'pod_name': 'Rmp', 'pod_flavor': 'medium-uni'}, {'pod_name': 'Ipp', 'pod_flavor': 'small'}]}\n",
      "\n",
      "👋 Exiting... Have a great day!\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chat_models import ChatOllama\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from colorama import Fore, Style, init  \n",
    "import re\n",
    "\n",
    "init()  # Initialize colorama  \n",
    "\n",
    "# --- Initialize LLM and Prompt ---\n",
    "llm = GapLLM()\n",
    "prompt_template = ChatPromptTemplate.from_template(prompt8)\n",
    "chain = prompt_template | llm | StrOutputParser()\n",
    "\n",
    "# --- In-memory storage for Q&A ---\n",
    "qa_history = []  # stores (q, a)\n",
    "\n",
    "df_result = None\n",
    "df_agent=False\n",
    "pf_agent=False\n",
    "\n",
    "# For cumulative tracking\n",
    "token_counts_per_turn = []\n",
    "total_tokens = 0\n",
    "\n",
    "# --- Chat loop ---\n",
    "print(f\"{Fore.YELLOW}🚀 AI Assistant is ready! Type 'exit' to stop.{Style.RESET_ALL}\")\n",
    "# print(prompt_template)\n",
    "while True:\n",
    "    question = input(\"Q: \")\n",
    "    if question.lower() in ['exit', 'quit', 'stop']:\n",
    "        print(f\"{Fore.YELLOW}👋 Exiting... Have a great day!{Style.RESET_ALL}\")  \n",
    "        break\n",
    "\n",
    "    # **Display Question**  \n",
    "    # print(f\"\\n{Fore.YELLOW}🔍 Question: {question}\\n{Style.RESET_ALL}\")  \n",
    "    retrieved_context = \"\"\n",
    "    # Step 1: Retrieve the most similar context using FAISS (assuming db_index is set up)\n",
    "    preprocess_data=False\n",
    "    chosendb = route_query(question)\n",
    "    if(df_result==None and chosendb == dfdbIndex):\n",
    "        df_agent=True\n",
    "        print(f\"{Fore.CYAN}📚 Context Source: Dimensioning Database{Style.RESET_ALL}\")  \n",
    "        # dimension_flavor_context =extract_dimension_flavor_info2(question)\n",
    "        dimension_flavor_context= extract_documents_from_query(df_map_list, question)\n",
    "        print(dimension_flavor_context)\n",
    "        retrieved_context = dict_to_context(dimension_flavor_context) if dimension_flavor_context else \"\"\n",
    "        preprocess_data=True\n",
    "        print(retrieved_context)\n",
    "    elif(chosendb == pfdbIndex):\n",
    "        pf_agent=True\n",
    "        print(f\"{Fore.CYAN}📚 Context Source: Pod Flavors Database{Style.RESET_ALL}\")  \n",
    "        retrieved_context = extract_pod_flavor_info(df_result)\n",
    "\n",
    "    # **Check DR Rules**  \n",
    "    dr_keyword = \"pod placement\"\n",
    "    if(dr_keyword in question):\n",
    "        print(f\"{Fore.RED}⚠️ DR Rules Triggered: '{dr_keyword}' detected!{Style.RESET_ALL}\") \n",
    "        with open(\"dr_rules_revamped.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "            dr_rules = f.read()\n",
    "        retrieved_context = dr_rules\n",
    "\n",
    "    # **Show Context**  \n",
    "    # print(f\"\\n{Fore.CYAN}📄 Context:{Style.RESET_ALL}\\n{retrieved_context}\\n\")\n",
    "    \n",
    "    # Step 2: Get the page content of the most similar document\n",
    "    # retrieved_context = context[0].page_content if context else \"\"\n",
    "\n",
    "    # Step 2: Build the chat history (all Q&A pairs so far)\n",
    "    chat_history = \"\\n\".join([f\"Q: {q}\\nA: {a}\" for q, a in qa_history])\n",
    "    # chat_history = \"\"\n",
    "\n",
    "    # prompt to count no. of tokens\n",
    "    # Prepare prompt as seen by LLM (tweak if your template differs)\n",
    "    prompt_for_token_count = (\n",
    "        f\"{retrieved_context}\\n\"\n",
    "        f\"{chat_history}\\n\"\n",
    "        f\"Q: {question}\\nA:\"\n",
    "    )\n",
    "    # Count tokens in the prompt\n",
    "    token_count = num_tokens(prompt_for_token_count)\n",
    "    token_counts_per_turn.append(token_count)\n",
    "    total_tokens+= token_count\n",
    "    print(f\"[Debug] Input Tokens This Turn: {token_count}\")\n",
    "    print(f\"[Debug] Total Input Tokens This Session: {total_tokens}\")\n",
    "    print(f\"[Debug] Token usage per question: {token_counts_per_turn}\") \n",
    "    # ------------------------\n",
    "\n",
    "    # Step 3: Get the answer from the LLM using the current question, retrieved context, and chat history\n",
    "    if(df_agent or pf_agent):\n",
    "        response= retrieved_context\n",
    "    else:\n",
    "        response = chain.invoke({\n",
    "            \"retrieved_context\": retrieved_context,\n",
    "            \"chat_history\": chat_history,\n",
    "            \"question\": question\n",
    "        })\n",
    "        # response = re.sub(r\"<think>.*?</think>\", \"\", response, flags=re.DOTALL).strip()\n",
    "        response = response.split(\"</think>\", 1)[1].strip()\n",
    "\n",
    "    if(preprocess_data):\n",
    "        df_result = preprocess_df_data(response)\n",
    "        preprocess_data=False\n",
    "        print(\"df-results\",df_result)\n",
    "\n",
    "\n",
    "    # Step 4: Store the current Q&A pair in history\n",
    "    qa_history.append((question, response))\n",
    "    df_agent=False\n",
    "    pf_agent=False\n",
    "\n",
    "    # **Display Answer**  \n",
    "    print(f\"{Fore.YELLOW}\\n💬 Answer:{Style.RESET_ALL}\\n{response}\\n{Fore.MAGENTA}{'-'*50}{Style.RESET_ALL}\")   \n",
    "\n",
    "    # **Display POD Info**  \n",
    "    if df_result is not None:  \n",
    "        print(f\"\\n{Fore.BLUE}🔍 Dimensioning Flavor Result: {df_result}{Style.RESET_ALL}\\n\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "OUTPUTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🚀 AI Assistant is ready! Type 'exit' to stop.\n",
    "📚 Context Source: Dimensioning Database\n",
    "[Debug] Input Tokens This Turn: 254\n",
    "[Debug] Total Input Tokens This Session: 254\n",
    "[Debug] Token usage per question: [254]\n",
    "c:\\Users\\aakash.a1\\AppData\\Local\\anaconda3\\envs\\rag-env\\lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'api.gap-srib.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
    "  warnings.warn(\n",
    "\n",
    "💬 Answer:\n",
    "Assistant:\n",
    "</think>\n",
    "\n",
    "---  \n",
    "Dimensioning Flavor: medium-regular-spr-t21  \n",
    "Network Function: uADPF  \n",
    "Pods and Pod Flavors:  \n",
    "1. DPP: fdd-270m-18c-gsm-6trx-spr  \n",
    "2. DIP: medium-uni  \n",
    "3. DMP: medium-uni  \n",
    "4. CMP: medium-uni  \n",
    "5. PMP: medium-uni  \n",
    "6. RMP: medium-uni  \n",
    "7. IPP: medium  \n",
    "---\n",
    "--------------------------------------------------\n",
    "\n",
    "🔍 Dimensioning Flavor Result: {'dimensioning_flavor': 'medium-regular-spr-t21', 'network_function': 'uADPF', 'pods': [{'pod_name': 'DPP', 'pod_flavor': 'fdd-270m-18c-gsm-6trx-spr'}, {'pod_name': 'DIP', 'pod_flavor': 'medium-uni'}, {'pod_name': 'DMP', 'pod_flavor': 'medium-uni'}, {'pod_name': 'CMP', 'pod_flavor': 'medium-uni'}, {'pod_name': 'PMP', 'pod_flavor': 'medium-uni'}, {'pod_name': 'RMP', 'pod_flavor': 'medium-uni'}, {'pod_name': 'IPP', 'pod_flavor': 'medium'}]}\n",
    "\n",
    "📚 Context Source: Pod Flavors Database\n",
    "[Debug] Input Tokens This Turn: 883\n",
    "[Debug] Total Input Tokens This Session: 1137\n",
    "[Debug] Token usage per question: [254, 883]\n",
    "c:\\Users\\aakash.a1\\AppData\\Local\\anaconda3\\envs\\rag-env\\lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'api.gap-srib.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
    "  warnings.warn(\n",
    "\n",
    "💬 Answer:\n",
    "**Resources for Each Pod Flavor:**  \n",
    "\n",
    "1. **DPP**: `fdd-270m-18c-gsm-6trx-spr`  \n",
    "   - vCPU Request: 52.0 | vCPU Limit: 52 | vMemory: 22.04 GB | Hugepage: 42.7 GB | Persistent Volume: 2(shared-pvc), 40(shared-log-pvc) | Package: 25A  \n",
    "\n",
    "2. **DIP**: `medium-uni`  \n",
    "   - vCPU Request: 2.0 | vCPU Limit: 3 | vMemory: 12.0 GB | Hugepage: Not Available | Persistent Volume: 2(shared-pvc), 40(shared-log-pvc) | Package: 25A  \n",
    "\n",
    "3. **DMP**: `medium-uni`  \n",
    "   - vCPU Request: 0.2 | vCPU Limit: 2 | vMemory: 2.0 GB | Hugepage: Not Available | Persistent Volume: 2(db-pvc), 4(shared-pvc), 40(shared-log-pvc) | Package: 25A  \n",
    "\n",
    "4. **CMP**: `medium-uni`  \n",
    "   - vCPU Request: 0.2 | vCPU Limit: 2 | vMemory: 2.0 GB | Hugepage: Not Available | Persistent Volume: 4(shared-pvc), 40(shared-log-pvc) | Package: 25A  \n",
    "\n",
    "5. **PMP**: `medium-uni`  \n",
    "   - vCPU Request: 0.1 | vCPU Limit: 2 | vMemory: 2.0 GB | Hugepage: Not Available | Persistent Volume: 4(shared-pvc), 40(shared-log-pvc) | Package: 25A  \n",
    "\n",
    "6. **RMP**: `medium-uni`  \n",
    "   - vCPU Request: 0.5 | vCPU Limit: 2 | vMemory: 2.0 GB | Hugepage: Not Available | Persistent Volume: 4(shared-pvc), 40(shared-log-pvc) | Package: 25A  \n",
    "\n",
    "7. **IPP**: `medium`  \n",
    "   - vCPU Request: 4.0 | vCPU Limit: 4 | vMemory: 2.0 GB | Hugepage: 2.0 GB | Persistent Volume: 2(shared-pvc), 40(shared-log-pvc) | Package: 25A\n",
    "--------------------------------------------------\n",
    "\n",
    "🔍 Dimensioning Flavor Result: {'dimensioning_flavor': 'medium-regular-spr-t21', 'network_function': 'uADPF', 'pods': [{'pod_name': 'DPP', 'pod_flavor': 'fdd-270m-18c-gsm-6trx-spr'}, {'pod_name': 'DIP', 'pod_flavor': 'medium-uni'}, {'pod_name': 'DMP', 'pod_flavor': 'medium-uni'}, {'pod_name': 'CMP', 'pod_flavor': 'medium-uni'}, {'pod_name': 'PMP', 'pod_flavor': 'medium-uni'}, {'pod_name': 'RMP', 'pod_flavor': 'medium-uni'}, {'pod_name': 'IPP', 'pod_flavor': 'medium'}]}\n",
    "\n",
    "📚 Context Source: Dimensioning Database\n",
    "⚠️ DR Rules Triggered: 'pod placement' detected!\n",
    "[Debug] Input Tokens This Turn: 2396\n",
    "[Debug] Total Input Tokens This Session: 3533\n",
    "[Debug] Token usage per question: [254, 883, 2396]\n",
    "c:\\Users\\aakash.a1\\AppData\\Local\\anaconda3\\envs\\rag-env\\lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'api.gap-srib.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
    "  warnings.warn(\n",
    "\n",
    "💬 Answer:\n",
    "\n",
    "--------------------------------------------------\n",
    "\n",
    "🔍 Dimensioning Flavor Result: {'dimensioning_flavor': 'medium-regular-spr-t21', 'network_function': 'uADPF', 'pods': [{'pod_name': 'DPP', 'pod_flavor': 'fdd-270m-18c-gsm-6trx-spr'}, {'pod_name': 'DIP', 'pod_flavor': 'medium-uni'}, {'pod_name': 'DMP', 'pod_flavor': 'medium-uni'}, {'pod_name': 'CMP', 'pod_flavor': 'medium-uni'}, {'pod_name': 'PMP', 'pod_flavor': 'medium-uni'}, {'pod_name': 'RMP', 'pod_flavor': 'medium-uni'}, {'pod_name': 'IPP', 'pod_flavor': 'medium'}]}\n",
    "\n",
    "📚 Context Source: Dimensioning Database\n",
    "⚠️ DR Rules Triggered: 'pod placement' detected!\n",
    "[Debug] Input Tokens This Turn: 2792\n",
    "[Debug] Total Input Tokens This Session: 6325\n",
    "[Debug] Token usage per question: [254, 883, 2396, 2792]\n",
    "c:\\Users\\aakash.a1\\AppData\\Local\\anaconda3\\envs\\rag-env\\lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'api.gap-srib.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
    "  warnings.warn(\n",
    "\n",
    "💬 Answer:\n",
    "#Calculation Total vCore Usage = Sum(Pod vCores) + CaaS(4) + Shared(2)\n",
    "**DPP**: 52 vcore ([Resource]) → Exceeds server capacity (48 vcore).\n",
    "Placement Failure: Insufficient server core number (required ≥58 vcore vs available 48 vcore).\n",
    "--------------------------------------------------\n",
    "\n",
    "🔍 Dimensioning Flavor Result: {'dimensioning_flavor': 'medium-regular-spr-t21', 'network_function': 'uADPF', 'pods': [{'pod_name': 'DPP', 'pod_flavor': 'fdd-270m-18c-gsm-6trx-spr'}, {'pod_name': 'DIP', 'pod_flavor': 'medium-uni'}, {'pod_name': 'DMP', 'pod_flavor': 'medium-uni'}, {'pod_name': 'CMP', 'pod_flavor': 'medium-uni'}, {'pod_name': 'PMP', 'pod_flavor': 'medium-uni'}, {'pod_name': 'RMP', 'pod_flavor': 'medium-uni'}, {'pod_name': 'IPP', 'pod_flavor': 'medium'}]}\n",
    "\n",
    "👋 Exiting... Have a great day!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
